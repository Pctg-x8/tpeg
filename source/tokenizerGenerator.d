module com.cterm2.tpeg.tokenizerGenerator;

import com.cterm2.tpeg.patternParser;
import com.cterm2.tpeg.tableStructure;
import settings = com.cterm2.tpeg.settings;
import std.stdio, std.path, std.array, std.range, std.algorithm;

class TokenizerGenerator
{
	File file;
	PatternParser parserEntity;

	public void run(PatternParser parser)
	{
		settings.acquireOutputDirectory();
		
		this.parserEntity = parser;
		this.file = File(buildPath(settings.OutputDirectory, parser.moduleName).withExtension("d").array, "w");
		this.initSource();
		this.file.writeln();
		this.file.writeln("public auto tokenize(string filePath){ return tokenizeStr(filePath.readText()); }");
		this.file.writeln("public auto tokenizeStr(string input)");
		this.file.writeln("{");
		this.expandShiftTable();
		this.file.writeln("\t", "return tokenList;");
		this.file.writeln("}");
		this.file.writeln();
		this.generateReduceHookers();
		this.file.close();
	}

	private void initSource()
	{
		void generateTokenEnumerator()
		{
			void generateTokenNameList(RangeT)(RangeT namelist)
			if(isInputRange!RangeT && is(ElementType!RangeT == string))
			{
				string lineStr = null;
				foreach(r; namelist)
				{
					// formatWrite
					if(lineStr.length >= 80 - 4)
					{
						this.file.writeln("\t", lineStr);
						lineStr = null;
					}
					lineStr ~= r ~ ", ";
				}
				this.file.writeln("\t", lineStr);
			}
			this.file.writeln("public enum EnumTokenType");
			this.file.writeln("{");
			generateTokenNameList(this.parserEntity.reduceTable.reduceTargetNames);
			foreach(s, t; this.parserEntity.specializes)
			{
				this.file.writeln("\n\t", "// Special Reduce for ", s);
				generateTokenNameList(t.map!(a => a.tokenName));
			}
			this.file.writeln("\n\t", "__INPUT_END__");
			this.file.writeln("}");
		}
		void generateLocationStructure()
		{
			this.file.writeln("public struct Location");
			this.file.writeln("{");
			this.file.writeln("\t", "size_t line, column;");
			this.file.writeln();
			this.file.writeln("\t", "public auto toString(){ return this.line.to!string ~ \":\" ~ this.column.to!string; }");
			this.file.writeln("}");
		}
		void generateTokenStructure()
		{
			this.file.writeln("public class Token");
			this.file.writeln("{");
			this.file.writeln("\t", "Location _loc;");
			this.file.writeln("\t", "EnumTokenType _type;");
			this.file.writeln("\t", "string _text;");
			this.file.writeln();
			this.file.writeln("\t", "public @property location(){ return this._loc; }");
			this.file.writeln("\t", "public @property type(){ return this._type; }");
			this.file.writeln("\t", "public @property text(){ return this._text; }");
			this.file.writeln();
			this.file.writeln("\t", "public this(Location l, EnumTokenType t)");
			this.file.writeln("\t", "{");
			this.file.writeln("\t\t", "this._loc = l;");
			this.file.writeln("\t\t", "this._type = t;");
			this.file.writeln("\t", "}");
			this.file.writeln("\t", "public this(Location l, EnumTokenType t, string tx)");
			this.file.writeln("\t", "{");
			this.file.writeln("\t\t", "this(l, t);");
			this.file.writeln("\t\t", "this._text = tx;");
			this.file.writeln("\t", "}");
			this.file.writeln();
			this.file.writeln("\t", "public @property dup(){ return new Token(this._loc, this._type, this._text); }");
			this.file.writeln("}");
		}
		void generateTokenizeError()
		{
			this.file.writeln("public class TokenizeError : Exception");
			this.file.writeln("{");
			this.file.writeln("\t", "public this(string err, Location loc){ super(err ~ \" at \" ~ loc.toString); }");
			this.file.writeln("}");
		}

		this.file.writeln("module ", (this.parserEntity.packageName ~ this.parserEntity.moduleName).join("."), ";");
		this.file.writeln();
		this.file.writeln("/// This file is generated by tpeg version 1.0 ///");
		this.file.writeln();
		this.file.writeln("import std.file;");
		this.file.writeln("import std.array, std.range;");
		this.file.writeln("import std.conv;");
		this.file.writeln();
		generateTokenEnumerator();
		generateLocationStructure();
		generateTokenStructure();
		generateTokenizeError();
	}
	private void expandShiftTable()
	{
		void generateForwardProcedure()
		{
			this.file.writeln("\t", "void forward()");
			this.file.writeln("\t", "{");
			this.file.writeln("\t\t", r"if(inputRange.front == '\n'){ loc.line++; loc.column = 1; }");
			this.file.writeln("\t\t", r"else if(inputRange.front == '\t') loc.column += loc.column % 4 + 1;");
			this.file.writeln("\t\t", "else loc.column++;");
			this.file.writeln("\t\t", "auto t = inputRange[1 .. $];");
			this.file.writeln("\t\t", "inputRange = t;");
			this.file.writeln("\t", "}");
		}

		this.file.writeln("\t", "Token[] tokenList = null;");
		this.file.writeln("\t", "auto loc = Location(1, 1), cloc = Location(0, 0);");
		this.file.writeln("\t", "auto inputRange = input[];");
		this.file.writeln("\t", "string inputHolder = null;");
		this.file.writeln("\t", "size_t currentState = 0;");
		generateForwardProcedure();
		this.file.writeln("\t", "while(!inputRange.empty)");
		this.file.writeln("\t", "{");
		this.file.writeln("\t\t", "switch(currentState)");
		this.file.writeln("\t\t", "{");
		foreach(i, n; this.parserEntity.shiftTable.stateList)
		{
			this.file.writeln("\t\t", "case ", i, ": ");
			if(n.nothing)
			{
				// empty rule
				this.file.writeln("\t\t\t", "throw new TokenizeError(\"Unrecognized character: \" ~ inputRange.front.to!string, loc);");
			}
			else if(n.wildcardOnly)
			{
				// default behavior
				this.generateBehavior(i, 3, n.wildcardAction);
			}
			else
			{
				this.file.writeln("\t\t\t", "switch(inputRange.front)");
				this.file.writeln("\t\t\t", "{");
				foreach(c; this.parserEntity.shiftTable.unescapedCandidates)
				{
					auto act = n.actionList[this.parserEntity.shiftTable.indexFromUnescapedCandidate(c)];
					if(act !is null)
					{
						if(c == "\\") this.file.writeln("\t\t\t", r"case '\\': ");
						else if(c == "'") this.file.writeln("\t\t\t", r"case '\'': ");
						else this.file.writeln("\t\t\t", "case '", c, "': ");
						this.generateBehavior(i, 4, act);
						this.file.writeln("\t\t\t\t", "break;");
					}
				}
				this.file.writeln("\t\t\t", "default: ");
				if(n.wildcardAction !is null)
				{
					this.generateBehavior(i, 4, n.wildcardAction);
					this.file.writeln("\t\t\t\t", "break;");
				}
				else
				{
					// empty rule
					this.file.writeln("\t\t\t\t", "throw new TokenizeError(\"Unrecognized character: \" ~ inputRange.front.to!string, loc);");
				}
				this.file.writeln("\t\t\t", "}");
			}
			this.file.writeln("\t\t\t", "break;");
		}
		this.file.writeln("\t\t", "default: assert(false); /* generator's bug. */");
		this.file.writeln("\t\t", "}");
		this.file.writeln("\t", "}");
		this.file.writeln("\t", "if(currentState != 0) throw new TokenizeError(\"Tokenizer is not terminated by \" ~ inputRange.front.to!string, loc);");
		this.file.writeln("\t", "tokenList ~= new Token(loc, EnumTokenType.__INPUT_END__);");
	}
	private void generateBehavior(size_t currentPhase, size_t tabDepth, TableActionBase act)
	{
		auto getTokenGenerateMethod(string reduceName, bool withShift)
		{
			immutable usingLocation = currentPhase == 0 ? "loc" : "cloc";
			immutable usingText = currentPhase == 0 && withShift ? "inputRange.front.to!string"
				: (withShift ? "inputHolder ~ inputRange.front.to!string" : "inputHolder");

			if(reduceName in this.parserEntity.specializes)
			{
				// use specialize
				return "reduceHooker_" ~ reduceName ~ "(" ~ usingLocation ~ ", " ~ usingText ~ ")";
			}
			else
			{
				return "new Token(" ~ usingLocation ~ ", EnumTokenType." ~ reduceName ~ ", " ~ usingText ~ ")";
			}
		}

		if(auto sh = cast(ShiftAction)act)
		{
			// shift and goto
			if(currentPhase == 0) this.file.writeln("\t".repeat(tabDepth).join, "cloc = loc;");
			this.file.writeln("\t".repeat(tabDepth).join, "inputHolder ~= inputRange.front;");
			this.file.writeln("\t".repeat(tabDepth).join, "forward();");
			if(sh.state != currentPhase) this.file.writeln("\t".repeat(tabDepth).join, "currentState = ", sh.state, ";");
		}
		else if(auto re = cast(ReduceAction)act)
		{
			if(re.isSkip)
			{
				// drop
				if(re.isShift)
				{
					this.file.writeln("\t".repeat(tabDepth).join, "forward();");
				}
				this.file.writeln("\t".repeat(tabDepth).join, "inputHolder = null;");
				this.file.writeln("\t".repeat(tabDepth).join, "currentState = 0;");
			}
			else
			{
				// reduce
				auto reduceName = this.parserEntity.reduceTable.reduceTargetName(re.reduceIndex);

				if(re.isShift)
				{
					// shift then reduce
					this.file.writeln("\t".repeat(tabDepth).join, "tokenList ~= ", getTokenGenerateMethod(reduceName, true), ";");
					this.file.writeln("\t".repeat(tabDepth).join, "forward();");
				}
				else
				{
					// reduce only(not expected in initial phase)
					assert(currentPhase != 0);
					// use cloc
					this.file.writeln("\t".repeat(tabDepth).join, "tokenList ~= ", getTokenGenerateMethod(reduceName, false), ";");
				}
				if(currentPhase != 0)
				{
					this.file.writeln("\t".repeat(tabDepth).join, "inputHolder = null;");
					this.file.writeln("\t".repeat(tabDepth).join, "currentState = 0;");
				}
			}
		}
		else assert(false);
	}
	void generateReduceHookers()
	{
		foreach(s, t; this.parserEntity.specializes)
		{
			this.file.writeln("auto reduceHooker_", s, "(Location loc, string text)");
			this.file.writeln("{");
			this.file.writeln("\t", "switch(text)");
			this.file.writeln("\t", "{");
			foreach(r; t)
			{
				this.file.writeln("\t", "case ", r.patternString, ":");
				this.file.writeln("\t\t", "return new Token(loc, EnumTokenType.", r.tokenName, ", text);");
			}
			this.file.writeln("\t", "default:");
			this.file.writeln("\t\t", "return new Token(loc, EnumTokenType.", s, ", text);");
			this.file.writeln("\t", "}");
			this.file.writeln("}");
		}
	}
}